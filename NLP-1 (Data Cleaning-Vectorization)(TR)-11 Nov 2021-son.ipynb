{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Oh man, this is pretty cool. We will do more such things.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh man, this is pretty cool.',\n",
       " 'we will do more such things.',\n",
       " \"don't, isn't, aren't, couldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_token = sent_tokenize(sample_text.lower())\n",
    "sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " '.',\n",
       " 'do',\n",
       " \"n't\",\n",
       " ',',\n",
       " 'is',\n",
       " \"n't\",\n",
       " ',',\n",
       " 'are',\n",
       " \"n't\",\n",
       " ',',\n",
       " 'could',\n",
       " \"n't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = word_tokenize(sample_text.lower())\n",
    "word_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " 'do',\n",
       " 'is',\n",
       " 'are',\n",
       " 'could']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalpha()] # .isalnum() for number and object\n",
    "tokens_without_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bunyaminkeles\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acaba',\n",
       " 'ama',\n",
       " 'aslında',\n",
       " 'az',\n",
       " 'bazı',\n",
       " 'belki',\n",
       " 'biri',\n",
       " 'birkaç',\n",
       " 'birşey',\n",
       " 'biz',\n",
       " 'bu',\n",
       " 'çok',\n",
       " 'çünkü',\n",
       " 'da',\n",
       " 'daha',\n",
       " 'de',\n",
       " 'defa',\n",
       " 'diye',\n",
       " 'eğer',\n",
       " 'en',\n",
       " 'gibi',\n",
       " 'hem',\n",
       " 'hep',\n",
       " 'hepsi',\n",
       " 'her',\n",
       " 'hiç',\n",
       " 'için',\n",
       " 'ile',\n",
       " 'ise',\n",
       " 'kez',\n",
       " 'ki',\n",
       " 'kim',\n",
       " 'mı',\n",
       " 'mu',\n",
       " 'mü',\n",
       " 'nasıl',\n",
       " 'ne',\n",
       " 'neden',\n",
       " 'nerde',\n",
       " 'nerede',\n",
       " 'nereye',\n",
       " 'niçin',\n",
       " 'niye',\n",
       " 'o',\n",
       " 'sanki',\n",
       " 'şey',\n",
       " 'siz',\n",
       " 'şu',\n",
       " 'tüm',\n",
       " 've',\n",
       " 'veya',\n",
       " 'ya',\n",
       " 'yani']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"turkish\")\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " 'do',\n",
       " 'is',\n",
       " 'are',\n",
       " 'could']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'things', 'could']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_without_sw = [t for t in tokens_without_punc if t not in stop_words] # if you make a sentiment analysis , you can remove \n",
    "                                                                          # negative auxiliary verb\n",
    "token_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in stop_words if \"n't\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bunyaminkeles\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'driven'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"driven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t) for t in token_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'thing', 'could']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"driving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = [PorterStemmer().stem(t) for t in token_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretti', 'cool', 'thing', 'could']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh man pretty cool thing could'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Function - NOT for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    \n",
    "   text_tokens = word_tokenize(data.lower())  #1. Tokenize\n",
    "    \n",
    "   tokens_without_punc = [w for w in text_tokens if w.isalpha()] #2. Remove Puncs\n",
    "   \n",
    "   tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words] #3. Removing Stopwords\n",
    "    \n",
    "   text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw] #4. lemma\n",
    "    \n",
    "   return \" \".join(text_cleaned) #joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oh man pretty cool thing could\n",
       "dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sample_text).apply(cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Function - for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh man, this is pretty cool. We will do more such things. don't, isn't, aren't, couldn't\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'We',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " 'dont',\n",
       " 'isnt',\n",
       " 'arent',\n",
       " 'couldnt']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# \\w typically matches [A-Za-z0-9_]\n",
    "s = re.sub('[^\\w\\s]','',sample_text) # ^ işareti diyor ki: metindeki A-Z,a-z, 0-9 olmayanları at!!\n",
    "word = word_tokenize(s)\n",
    "word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_fsa(data):\n",
    "    \n",
    "    import re\n",
    "    #1. Remove Puncs\n",
    "    # \\w typically matches [A-Za-z0-9_]\n",
    "    text = re.sub('[^\\w\\s]','',sample_text)\n",
    "         \n",
    "    #2. Tokenize\n",
    "    text_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #3. Remove numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "    \n",
    "    #4. Removing Stopwords\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #5. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oh man pretty cool thing dont isnt arent couldnt\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sample_text).apply(cleaning_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Steven Paul Jobs was an American business magnate, industrial designer, investor, and media proprietor. He was the chairman, chief executive officer (CEO), and co-founder of Apple Inc.; the chairman and majority shareholder of Pixar; a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of NeXT. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bunyaminkeles\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Oh', 'UH'),\n",
       " ('man', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('pretty', 'JJ'),\n",
       " ('cool', 'NN'),\n",
       " ('We', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('do', 'VB'),\n",
       " ('more', 'RBR'),\n",
       " ('such', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " ('dont', 'VBP'),\n",
       " ('isnt', 'JJ'),\n",
       " ('arent', 'NN'),\n",
       " ('couldnt', 'NN')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagg = pos_tag(tokens)\n",
    "tagg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Abbreviation\tMeaning\n",
    "\n",
    "CC\t            coordinating conjunction\n",
    "CD\t            cardinal digit\n",
    "DT\t            determiner\n",
    "EX\t            existential there\n",
    "FW\t            foreign word\n",
    "IN\t            preposition/subordinating conjunction\n",
    "JJ\t            This NLTK POS Tag is an adjective (large)\n",
    "JJR\t            adjective, comparative (larger)\n",
    "JJS\t            adjective, superlative (largest)\n",
    "LS\t            list market\n",
    "MD\t            modal (could, will)\n",
    "NN\t            noun, singular (cat, tree)\n",
    "NNS\t            noun plural (desks)\n",
    "NNP\t            proper noun, singular (sarah)\n",
    "NNPS\t        proper noun, plural (indians or americans)\n",
    "PDT\t            predeterminer (all, both, half)\n",
    "POS\t            possessive ending (parent\\ ‘s)\n",
    "PRP\t            personal pronoun (hers, herself, him, himself)\n",
    "PRP$\t        possessive pronoun (her, his, mine, my, our )\n",
    "RB\t            adverb (occasionally, swiftly)\n",
    "RBR\t            adverb, comparative (greater)\n",
    "RBS\t            adverb, superlative (biggest)\n",
    "RP\t            particle (about)\n",
    "TO\t            infinite marker (to)\n",
    "UH\t            interjection (goodbye)\n",
    "VB\t            verb (ask)\n",
    "VBG\t            verb gerund (judging)\n",
    "VBD\t            verb past tense (pleaded)\n",
    "VBN\t            verb past participle (reunified)\n",
    "VBP\t            verb, present tense not 3rd person singular(wrap)\n",
    "VBZ\t            verb, present tense with 3rd person singular (bases)\n",
    "WDT\t            wh-determiner (that, what)\n",
    "WP\t            wh- pronoun (who)\n",
    "WRB\t            wh- adverb (how)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                     [\n\u001b[1;32m--> 818\u001b[1;33m                         find_binary(\n\u001b[0m\u001b[0;32m    819\u001b[0m                             \u001b[1;34m\"gs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    687\u001b[0m ):\n\u001b[1;32m--> 688\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    689\u001b[0m         find_binary_iter(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m     for file in find_file_iter(\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    835\u001b[0m                 )\n\u001b[0;32m    836\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('Oh', 'UH'), ('man', 'NN'), ('this', 'DT'), ('is', 'VBZ'), ('pretty', 'JJ'), ('cool', 'NN'), ('We', 'PRP'), ('will', 'MD'), ('do', 'VB'), ('more', 'RBR'), ('such', 'JJ'), ('things', 'NNS'), ('dont', 'VBP'), ('isnt', 'JJ'), ('arent', 'NN'), ('couldnt', 'NN')])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = ne_chunk(tagg)\n",
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NE Türü         \tÖrnek\n",
    "\n",
    "ORGANIZATION    \tGeorgia-Pacific Corp., WHO\n",
    "PERSON          \tEddy Bonte, President Obama\n",
    "LOCATION        \tMurray River, Mount Everest\n",
    "DATE            \tJune, 2008-06-29\n",
    "TIME            \ttwo fifty a m, 1:30 p.m.\n",
    "MONEY           \t175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT         \ttwenty pct, 18.75 %\n",
    "FACILITY        \tWashington Monument, Stonehenge\n",
    "GPE             \tSouth East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorization and TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"airline_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                        0.0  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold      name negativereason_gold  retweet_count  \\\n",
       "0                    NaN   cairdin                 NaN              0   \n",
       "1                    NaN  jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment                                               text\n",
       "0               neutral                @VirginAmerica What @dhepburn said.\n",
       "1              positive  @VirginAmerica plus you've added commercials t...\n",
       "2               neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3              negative  @VirginAmerica it's really aggressive to blast...\n",
       "4              negative  @VirginAmerica and it's a really big bad thing...\n",
       "...                 ...                                                ...\n",
       "14635          positive  @AmericanAir thank you we got on a different f...\n",
       "14636          negative  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14637           neutral  @AmericanAir Please bring American Airlines to...\n",
       "14638          negative  @AmericanAir you have my money, you change my ...\n",
       "14639           neutral  @AmericanAir we have 8 ppl so we need 2 know h...\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['airline_sentiment','text']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "7           neutral  @VirginAmerica Really missed a prime opportuni..."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:8, :]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"text\"] = df2[\"text\"].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus added commercial experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica today must mean need take anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica seriously would pay flight seat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica yes nearly every time fly vx ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica really missed prime opportunity ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                        virginamerica dhepburn said\n",
       "1          positive  virginamerica plus added commercial experience...\n",
       "2           neutral  virginamerica today must mean need take anothe...\n",
       "3          negative  virginamerica really aggressive blast obnoxiou...\n",
       "4          negative                 virginamerica really big bad thing\n",
       "5          negative  virginamerica seriously would pay flight seat ...\n",
       "6          positive  virginamerica yes nearly every time fly vx ear...\n",
       "7           neutral  virginamerica really missed prime opportunity ..."
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2[\"text\"]\n",
    "y = df2[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "X_test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another',\n",
       " 'away',\n",
       " 'bad',\n",
       " 'big',\n",
       " 'dhepburn',\n",
       " 'ear',\n",
       " 'every',\n",
       " 'fly',\n",
       " 'go',\n",
       " 'mean',\n",
       " 'must',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'really',\n",
       " 'said',\n",
       " 'take',\n",
       " 'thing',\n",
       " 'time',\n",
       " 'today',\n",
       " 'trip',\n",
       " 'virginamerica',\n",
       " 'vx',\n",
       " 'worm',\n",
       " 'yes']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>ear</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  away  bad  big  dhepburn  ear  every  fly  go  mean  must  nearly  \\\n",
       "0        0     1    0    0         0    1      1    1   1     0     0       1   \n",
       "1        0     0    0    0         1    0      0    0   0     0     0       0   \n",
       "2        1     0    0    0         0    0      0    0   0     1     1       0   \n",
       "3        0     0    1    1         0    0      0    0   0     0     0       0   \n",
       "\n",
       "   need  really  said  take  thing  time  today  trip  virginamerica  vx  \\\n",
       "0     0       0     0     0      0     1      0     0              1   1   \n",
       "1     0       0     1     0      0     0      0     0              1   0   \n",
       "2     1       0     0     1      0     0      1     1              1   0   \n",
       "3     0       1     0     0      1     0      0     0              1   0   \n",
       "\n",
       "   worm  yes  \n",
       "0     1    1  \n",
       "1     0    0  \n",
       "2     0    0  \n",
       "3     0    0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx ear worm go away'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'virginamerica': 20,\n",
       " 'yes': 23,\n",
       " 'nearly': 11,\n",
       " 'every': 6,\n",
       " 'time': 17,\n",
       " 'fly': 7,\n",
       " 'vx': 21,\n",
       " 'ear': 5,\n",
       " 'worm': 22,\n",
       " 'go': 8,\n",
       " 'away': 1,\n",
       " 'dhepburn': 4,\n",
       " 'said': 14,\n",
       " 'today': 18,\n",
       " 'must': 10,\n",
       " 'mean': 9,\n",
       " 'need': 12,\n",
       " 'take': 15,\n",
       " 'another': 0,\n",
       " 'trip': 19,\n",
       " 'really': 13,\n",
       " 'big': 3,\n",
       " 'bad': 2,\n",
       " 'thing': 16}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn TD-IDF\n",
    "https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another',\n",
       " 'away',\n",
       " 'bad',\n",
       " 'big',\n",
       " 'dhepburn',\n",
       " 'ear',\n",
       " 'every',\n",
       " 'fly',\n",
       " 'go',\n",
       " 'mean',\n",
       " 'must',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'really',\n",
       " 'said',\n",
       " 'take',\n",
       " 'thing',\n",
       " 'time',\n",
       " 'today',\n",
       " 'trip',\n",
       " 'virginamerica',\n",
       " 'vx',\n",
       " 'worm',\n",
       " 'yes']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.31200802, 0.        , 0.        , 0.        ,\n",
       "        0.31200802, 0.31200802, 0.31200802, 0.31200802, 0.        ,\n",
       "        0.        , 0.31200802, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31200802, 0.        , 0.        ,\n",
       "        0.16281873, 0.31200802, 0.31200802, 0.31200802],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.66338461,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.66338461,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34618161, 0.        , 0.        , 0.        ],\n",
       "       [0.37082034, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.37082034,\n",
       "        0.37082034, 0.        , 0.37082034, 0.        , 0.        ,\n",
       "        0.37082034, 0.        , 0.        , 0.37082034, 0.37082034,\n",
       "        0.19350944, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.48380259, 0.48380259, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.48380259, 0.        ,\n",
       "        0.        , 0.48380259, 0.        , 0.        , 0.        ,\n",
       "        0.25246826, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>ear</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.162819</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.346182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.193509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.252468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another      away       bad       big  dhepburn       ear     every  \\\n",
       "0  0.00000  0.312008  0.000000  0.000000  0.000000  0.312008  0.312008   \n",
       "1  0.00000  0.000000  0.000000  0.000000  0.663385  0.000000  0.000000   \n",
       "2  0.37082  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.00000  0.000000  0.483803  0.483803  0.000000  0.000000  0.000000   \n",
       "\n",
       "        fly        go     mean     must    nearly     need    really  \\\n",
       "0  0.312008  0.312008  0.00000  0.00000  0.312008  0.00000  0.000000   \n",
       "1  0.000000  0.000000  0.00000  0.00000  0.000000  0.00000  0.000000   \n",
       "2  0.000000  0.000000  0.37082  0.37082  0.000000  0.37082  0.000000   \n",
       "3  0.000000  0.000000  0.00000  0.00000  0.000000  0.00000  0.483803   \n",
       "\n",
       "       said     take     thing      time    today     trip  virginamerica  \\\n",
       "0  0.000000  0.00000  0.000000  0.312008  0.00000  0.00000       0.162819   \n",
       "1  0.663385  0.00000  0.000000  0.000000  0.00000  0.00000       0.346182   \n",
       "2  0.000000  0.37082  0.000000  0.000000  0.37082  0.37082       0.193509   \n",
       "3  0.000000  0.00000  0.483803  0.000000  0.00000  0.00000       0.252468   \n",
       "\n",
       "         vx      worm       yes  \n",
       "0  0.312008  0.312008  0.312008  \n",
       "1  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx ear worm go away'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "another          0.370820\n",
       "mean             0.370820\n",
       "trip             0.370820\n",
       "today            0.370820\n",
       "take             0.370820\n",
       "must             0.370820\n",
       "need             0.370820\n",
       "virginamerica    0.193509\n",
       "fly              0.000000\n",
       "thing            0.000000\n",
       "worm             0.000000\n",
       "vx               0.000000\n",
       "bad              0.000000\n",
       "big              0.000000\n",
       "time             0.000000\n",
       "dhepburn         0.000000\n",
       "go               0.000000\n",
       "said             0.000000\n",
       "really           0.000000\n",
       "away             0.000000\n",
       "nearly           0.000000\n",
       "ear              0.000000\n",
       "every            0.000000\n",
       "yes              0.000000\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names()).loc[2].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterthemes in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: notebook>=5.6.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (6.3.0)\n",
      "Requirement already satisfied: ipython>=5.4.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (7.22.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (3.3.4)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (4.7.1)\n",
      "Requirement already satisfied: lesscpy>=0.11.2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (0.15.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (52.0.0.post20210125)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (5.0.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.17.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (2.8.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (5.0.6)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.4.1->jupyterthemes) (0.7.0)\n",
      "Requirement already satisfied: six in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.15.0)\n",
      "Requirement already satisfied: ply in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.20.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (20.0.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.0.7)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.3.4)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (20.1.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.10.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.9.4)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.12)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (1.5.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.3)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.3)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterthemes) (227)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=5.6.0->jupyterthemes) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=5.6.0->jupyterthemes) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.6.0->jupyterthemes) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (2.0.1)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.1.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (3.3.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.5.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.10)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (20.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (20.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterthemes in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: ipython>=5.4.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (7.22.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (4.7.1)\n",
      "Requirement already satisfied: notebook>=5.6.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (6.3.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (3.3.4)\n",
      "Requirement already satisfied: lesscpy>=0.11.2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyterthemes) (0.15.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.17.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (5.0.5)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (2.8.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (5.0.6)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (52.0.0.post20210125)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.17)\n",
      "Requirement already satisfied: backcall in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.4.1->jupyterthemes) (0.7.0)\n",
      "Requirement already satisfied: six in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.15.0)\n",
      "Requirement already satisfied: ply in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.20.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.3)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (20.0.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.1)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.0.7)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.9.4)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.12)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.10.1)\n",
      "Requirement already satisfied: nbformat in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.3)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.3.4)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (20.1.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterthemes) (227)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=5.6.0->jupyterthemes) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=5.6.0->jupyterthemes) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.6.0->jupyterthemes) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (2.0.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (3.3.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.3)\n",
      "Requirement already satisfied: testpath in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.3)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.7.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.5.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (20.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\bunyaminkeles\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (20.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Themes: \n",
      "   chesterish\n",
      "   grade3\n",
      "   gruvboxd\n",
      "   gruvboxl\n",
      "   monokai\n",
      "   oceans16\n",
      "   onedork\n",
      "   solarizedd\n",
      "   solarizedl\n"
     ]
    }
   ],
   "source": [
    "### Dark theme for jupyter notebook, no need to run.\n",
    "!pip install jupyterthemes\n",
    "!pip install --upgrade jupyterthemes\n",
    "!jt -l\n",
    "#emir's choice\n",
    "!jt -t chesterish -T -N -kl -tf ptsans -tfs 13 -f fira -fs 11 -ofs 10 -nf ptsans -nfs 13 -cursw 2 -cursc r -cellw 88% -vim\n",
    "from jupyterthemes import jtplot\n",
    "# choose which theme to inherit plotting style from\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "jtplot.style(theme='chesterish', ticks=True,)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtplot.style(theme='chesterish', ticks=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
